{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=pd.read_csv(\"smatbot.csv\",sep=\"^\",names=[\"text\",\"rpl\"])\n",
    "lines=lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rpl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tshirt</td>\n",
       "      <td>&lt;SOS&gt; okay how many dress &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add tshirt</td>\n",
       "      <td>&lt;SOS&gt; okay how many dress &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need tshirt</td>\n",
       "      <td>&lt;SOS&gt; okay how many dress &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount available kya</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any discount</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>offer</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is there any discount</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>how to apply for offers</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>where i can find the coupon code</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>why is everyone commenting their b dates are w...</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>are there any discounts available</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>are there any offers available on this</td>\n",
       "      <td>&lt;SOS&gt; there are only limited products which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>can i get 4xl tshirt with july quotes</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>if 4 xxl size available so please call me</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>size please</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i want a 6xl t shirtis it available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hii use formal shirt 42  what will be my size ...</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>size not getting</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3xl available hai</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>is l size available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>what about the sizes</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>what all sizes are available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>is xxl available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>is small size available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>is m size available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>is large size available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>is medium size available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>where do i find the size selection where is th...</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>is xl available</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest size  visit our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>can u people deliver this tshirt jammu and kas...</td>\n",
       "      <td>&lt;SOS&gt; we service 8000 pincodes via cod please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>how many price</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest price  visit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>plz send me inbox of prise or how to purchase</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest price  visit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>how much is this</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest price  visit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>what is the price please</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest price  visit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>how much price buddy</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest price  visit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>how much is his available in the philippines</td>\n",
       "      <td>&lt;SOS&gt; thanks for your interest price  visit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>my what is aap number</td>\n",
       "      <td>&lt;SOS&gt; no we do not have any whatsapp number yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>plz price 9215521210 whatsup</td>\n",
       "      <td>&lt;SOS&gt; no we do not have any whatsapp number yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>whatsapp number milega kya</td>\n",
       "      <td>&lt;SOS&gt; no we do not have any whatsapp number yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>very nice jacket my what is aap number 9816146689</td>\n",
       "      <td>&lt;SOS&gt; no we do not have any whatsapp number yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>how many plzmy wtapp n 9672820159</td>\n",
       "      <td>&lt;SOS&gt; no we do not have any whatsapp number yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>shop address pls</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>kidhar milenge yeh t shirt address do</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>is it available in hyderabad india</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>shop kaha h apkiplz contact no</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>kaha milega</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>please send me address</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>kha par hbye store</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>where is shop</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>your address</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>where is your store brother</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>where is store</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>pls tell where it is available</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>address nd price</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>i need to buy pls help me out in gujrat</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>where did buy it</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>plz share address where it is available</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>locationplz</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>up mein shop kaha hain</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>kese kharidenge shop pr kab tak milne lagegi</td>\n",
       "      <td>&lt;SOS&gt; we do not have any physical store you ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1781 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1                                                tshirt   \n",
       "2                                            add tshirt   \n",
       "3                                         i need tshirt   \n",
       "4                                discount available kya   \n",
       "5                                          any discount   \n",
       "6                                                offer    \n",
       "7                                 is there any discount   \n",
       "8                               how to apply for offers   \n",
       "9                      where i can find the coupon code   \n",
       "10    why is everyone commenting their b dates are w...   \n",
       "11                    are there any discounts available   \n",
       "12               are there any offers available on this   \n",
       "13                can i get 4xl tshirt with july quotes   \n",
       "14            if 4 xxl size available so please call me   \n",
       "15                                          size please   \n",
       "16                  i want a 6xl t shirtis it available   \n",
       "17    hii use formal shirt 42  what will be my size ...   \n",
       "18                                     size not getting   \n",
       "19                                    3xl available hai   \n",
       "20                                  is l size available   \n",
       "21                                what about the sizes    \n",
       "22                         what all sizes are available   \n",
       "23                                     is xxl available   \n",
       "24                              is small size available   \n",
       "25                                  is m size available   \n",
       "26                              is large size available   \n",
       "27                             is medium size available   \n",
       "28    where do i find the size selection where is th...   \n",
       "29                                      is xl available   \n",
       "30    can u people deliver this tshirt jammu and kas...   \n",
       "...                                                 ...   \n",
       "1752                                     how many price   \n",
       "1753      plz send me inbox of prise or how to purchase   \n",
       "1754                                   how much is this   \n",
       "1755                           what is the price please   \n",
       "1756                               how much price buddy   \n",
       "1757       how much is his available in the philippines   \n",
       "1758                              my what is aap number   \n",
       "1759                       plz price 9215521210 whatsup   \n",
       "1760                         whatsapp number milega kya   \n",
       "1761  very nice jacket my what is aap number 9816146689   \n",
       "1762                  how many plzmy wtapp n 9672820159   \n",
       "1763                                   shop address pls   \n",
       "1764              kidhar milenge yeh t shirt address do   \n",
       "1765                 is it available in hyderabad india   \n",
       "1766                     shop kaha h apkiplz contact no   \n",
       "1767                                        kaha milega   \n",
       "1768                             please send me address   \n",
       "1769                                kha par hbye store    \n",
       "1770                                      where is shop   \n",
       "1771                                       your address   \n",
       "1772                        where is your store brother   \n",
       "1773                                    where is store    \n",
       "1774                     pls tell where it is available   \n",
       "1775                                   address nd price   \n",
       "1776            i need to buy pls help me out in gujrat   \n",
       "1777                                   where did buy it   \n",
       "1778            plz share address where it is available   \n",
       "1779                                       locationplz    \n",
       "1780                             up mein shop kaha hain   \n",
       "1781      kese kharidenge shop pr kab tak milne lagegi    \n",
       "\n",
       "                                                    rpl  \n",
       "1                       <SOS> okay how many dress <EOS>  \n",
       "2                       <SOS> okay how many dress <EOS>  \n",
       "3                       <SOS> okay how many dress <EOS>  \n",
       "4     <SOS> there are only limited products which ar...  \n",
       "5     <SOS> there are only limited products which ar...  \n",
       "6     <SOS> there are only limited products which ar...  \n",
       "7     <SOS> there are only limited products which ar...  \n",
       "8     <SOS> there are only limited products which ar...  \n",
       "9     <SOS> there are only limited products which ar...  \n",
       "10    <SOS> there are only limited products which ar...  \n",
       "11    <SOS> there are only limited products which ar...  \n",
       "12    <SOS> there are only limited products which ar...  \n",
       "13    <SOS> thanks for your interest size  visit our...  \n",
       "14    <SOS> thanks for your interest size  visit our...  \n",
       "15    <SOS> thanks for your interest size  visit our...  \n",
       "16    <SOS> thanks for your interest size  visit our...  \n",
       "17    <SOS> thanks for your interest size  visit our...  \n",
       "18    <SOS> thanks for your interest size  visit our...  \n",
       "19    <SOS> thanks for your interest size  visit our...  \n",
       "20    <SOS> thanks for your interest size  visit our...  \n",
       "21    <SOS> thanks for your interest size  visit our...  \n",
       "22    <SOS> thanks for your interest size  visit our...  \n",
       "23    <SOS> thanks for your interest size  visit our...  \n",
       "24    <SOS> thanks for your interest size  visit our...  \n",
       "25    <SOS> thanks for your interest size  visit our...  \n",
       "26    <SOS> thanks for your interest size  visit our...  \n",
       "27    <SOS> thanks for your interest size  visit our...  \n",
       "28    <SOS> thanks for your interest size  visit our...  \n",
       "29    <SOS> thanks for your interest size  visit our...  \n",
       "30    <SOS> we service 8000 pincodes via cod please ...  \n",
       "...                                                 ...  \n",
       "1752  <SOS> thanks for your interest price  visit ou...  \n",
       "1753  <SOS> thanks for your interest price  visit ou...  \n",
       "1754  <SOS> thanks for your interest price  visit ou...  \n",
       "1755  <SOS> thanks for your interest price  visit ou...  \n",
       "1756  <SOS> thanks for your interest price  visit ou...  \n",
       "1757  <SOS> thanks for your interest price  visit ou...  \n",
       "1758  <SOS> no we do not have any whatsapp number yo...  \n",
       "1759  <SOS> no we do not have any whatsapp number yo...  \n",
       "1760  <SOS> no we do not have any whatsapp number yo...  \n",
       "1761  <SOS> no we do not have any whatsapp number yo...  \n",
       "1762  <SOS> no we do not have any whatsapp number yo...  \n",
       "1763  <SOS> we do not have any physical store you ca...  \n",
       "1764  <SOS> we do not have any physical store you ca...  \n",
       "1765  <SOS> we do not have any physical store you ca...  \n",
       "1766  <SOS> we do not have any physical store you ca...  \n",
       "1767  <SOS> we do not have any physical store you ca...  \n",
       "1768  <SOS> we do not have any physical store you ca...  \n",
       "1769  <SOS> we do not have any physical store you ca...  \n",
       "1770  <SOS> we do not have any physical store you ca...  \n",
       "1771  <SOS> we do not have any physical store you ca...  \n",
       "1772  <SOS> we do not have any physical store you ca...  \n",
       "1773  <SOS> we do not have any physical store you ca...  \n",
       "1774  <SOS> we do not have any physical store you ca...  \n",
       "1775  <SOS> we do not have any physical store you ca...  \n",
       "1776  <SOS> we do not have any physical store you ca...  \n",
       "1777  <SOS> we do not have any physical store you ca...  \n",
       "1778  <SOS> we do not have any physical store you ca...  \n",
       "1779  <SOS> we do not have any physical store you ca...  \n",
       "1780  <SOS> we do not have any physical store you ca...  \n",
       "1781  <SOS> we do not have any physical store you ca...  \n",
       "\n",
       "[1781 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lower casing all characters\n",
    "lines.text=lines.text.apply(lambda x:str(x))\n",
    "lines.rpl=lines.rpl.apply(lambda x:str(x))\n",
    "lines.text=lines.text.apply(lambda x:x.lower())\n",
    "lines.rpl=lines.rpl.apply(lambda x:x.lower())\n",
    "\n",
    "def decontract(sentence):\n",
    "    sentence = re.sub('\\s+', ' ',sentence).strip()\n",
    "    #sentence = re.sub(r\"(\\?)+\", \" ?\",sentence)\n",
    "    sentence = re.sub(r\"won\\'t\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    #sentence = re.sub(r'(\\.\\.+)',\"\",sentence)\n",
    "    return sentence\n",
    "\n",
    "#converting can't to cannot etc\n",
    "lines.text=lines.text.apply(lambda x:decontract(x))\n",
    "lines.rpl=lines.rpl.apply(lambda x:decontract(x))\n",
    "\n",
    "#removing double quotations\n",
    "lines.text=lines.text.apply(lambda x:re.sub(\"'\",\"\",x))\n",
    "lines.rpl=lines.rpl.apply(lambda x:re.sub(\"'\",\"\",x))\n",
    "lines.text=lines.text.apply(lambda x:re.sub('\"',\"\",x))\n",
    "lines.rpl=lines.rpl.apply(lambda x:re.sub('\"',\"\",x))\n",
    "\n",
    "#removing all special characters\n",
    "exclude=set(string.punctuation)\n",
    "lines.text=lines.text.apply(lambda x:\"\".join(ch for ch in x if ch not in exclude))\n",
    "lines.rpl=lines.rpl.apply(lambda x:\"\".join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "#removing numbers\n",
    "#lines.text=lines.text.apply(lambda x:re.sub(\"[0123456789]\",\"\",x))\n",
    "#lines.rpl = lines.rpl.apply(lambda x: re.sub(\"[0123456789]\", \"\", x))\n",
    "\n",
    "#adding start and end tokens\n",
    "lines.rpl=lines.rpl.apply(lambda x:\"<SOS> \"+x+\" <EOS>\")\n",
    "lines.sample(7)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words in texts 1058\n",
      "total number of words in replies 211\n",
      "Maximum input sequence length 41\n",
      "Maximum output sequence length 59\n"
     ]
    }
   ],
   "source": [
    "text_w=set()\n",
    "ileng=0\n",
    "for line in lines.text:\n",
    "    lenofs=len(line.split())\n",
    "    if lenofs>ileng:\n",
    "        ileng=lenofs\n",
    "    for word in line.split():\n",
    "        if word not in text_w and len(word)>0:\n",
    "            text_w.add(word)\n",
    "rpl_w=set()\n",
    "oleng=0\n",
    "for line in lines.rpl:\n",
    "    lenofs=len(line.split())\n",
    "    if lenofs>oleng:\n",
    "        oleng=lenofs\n",
    "    for word in line.split():\n",
    "        if word not in rpl_w:\n",
    "            rpl_w.add(word)\n",
    "print(\"total number of words in texts {}\".format(len(text_w)))\n",
    "print(\"total number of words in replies {}\".format(len(rpl_w)))\n",
    "print(\"Maximum input sequence length {}\".format(ileng))\n",
    "print(\"Maximum output sequence length {}\".format(oleng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1059, 212)\n"
     ]
    }
   ],
   "source": [
    "input_words=sorted(list(text_w))\n",
    "target_words=sorted(list(rpl_w))\n",
    "num_encoder_tokens=len(text_w)\n",
    "num_decoder_tokens=len(rpl_w)\n",
    "num_decoder_tokens+=1\n",
    "num_encoder_tokens+=1\n",
    "print((num_encoder_tokens,num_decoder_tokens))\n",
    "# mapping words with indexes\n",
    "#input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "#target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1424,), (357,))\n"
     ]
    }
   ],
   "source": [
    "#pre-trained word embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "inputs=lines.text.tolist()\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(inputs)\n",
    "input_sequences=tokenizer.texts_to_sequences(inputs)\n",
    "input_word_index=tokenizer.word_index\n",
    "\n",
    "targets=lines.rpl.tolist()\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(targets)\n",
    "target_sequences=tokenizer.texts_to_sequences(targets)\n",
    "target_word_index=tokenizer.word_index\n",
    "\n",
    "lines = shuffle(lines)\n",
    "X,y=lines.text,lines.rpl\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "print((X_train.shape, X_test.shape))\n",
    "X_train.to_pickle('weights_lstm/X_train.pkl')\n",
    "X_test.to_pickle('weights_lstm/X_test.pkl')\n",
    "target_word_index[\"<SOS>\"]=1\n",
    "target_word_index[\"<EOS>\"]=2\n",
    "del target_word_index[\"sos\"]\n",
    "del target_word_index[\"eos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '/root/Downloads/glove.6B'\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size=300\n",
    "input_embedding_matrix=np.zeros((len(input_word_index)+1,embedding_size))\n",
    "for word,i in input_word_index.items():\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is None:\n",
    "        input_embedding_matrix[i]=np.zeros((embedding_size,))\n",
    "    else:\n",
    "        input_embedding_matrix[i]=embedding_vector\n",
    "target_embedding_matrix=np.zeros((len(target_word_index)+1,embedding_size))\n",
    "for word,i in target_word_index.items():\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is None:\n",
    "        target_embedding_matrix[i]=np.zeros((embedding_size,))\n",
    "    else:\n",
    "        target_embedding_matrix[i]=embedding_vector\n",
    "target_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "def generate_batch(X=X_train,y=y_train,batch_size=128):\n",
    "    global cnt\n",
    "    while True:\n",
    "        for j in range(0,len(X),batch_size):\n",
    "            #defining encoder and target inputs and outputs matrices\n",
    "            # {ileng}--max length of input\n",
    "            # {oleng}--max length of output\n",
    "            encoder_input_data=np.zeros((batch_size,ileng),dtype=\"float32\")\n",
    "            decoder_input_data=np.zeros((batch_size,oleng),dtype=\"float32\")\n",
    "            decoder_target_data=np.zeros((batch_size,oleng,num_decoder_tokens),dtype=\"float32\")\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size],y[j:j+batch_size])):\n",
    "                #print(input_text,target_text)\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_word_index[word]\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word_index[word]\n",
    "                    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                    if t > 0:\n",
    "                        # decoder_target_data will be ahead by one timestep\n",
    "                        # and will not include the start character.\n",
    "                        decoder_target_data[i, t - 1, target_word_index[word]] = 1\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 300)    317700      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 300)    63600       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 300), (None, 721200      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 300),  721200      embedding_5[0][0]                \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 212)    63812       lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,887,512\n",
      "Trainable params: 1,887,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder model layers\n",
    "batch_size=32\n",
    "encoder_inputs=Input(shape=(None,))\n",
    "encoder_emb=Embedding(len(input_word_index)+1,embedding_size,input_length=batch_size)(encoder_inputs)\n",
    "encoder_lstm=LSTM(embedding_size,return_state=True)\n",
    "encoder_outputs,state_h,state_c=encoder_lstm(encoder_emb)\n",
    "encoder_states=[state_h,state_c]\n",
    "#we discard ouputs of encoders as we only need he final state\n",
    "#to feed into decoder network\n",
    "\n",
    "#decoder model layer\n",
    "decoder_inputs=Input(shape=(None,))\n",
    "decoder_emb_layer=Embedding(len(target_word_index)+1,embedding_size,input_length=batch_size)\n",
    "decoder_emb=decoder_emb_layer(decoder_inputs)\n",
    "decoder_lstm=LSTM(embedding_size, return_sequences=True, return_state=True)\n",
    "decoder_outputs,_,_=decoder_lstm(decoder_emb,initial_state=encoder_states)\n",
    "decoder_dense=Dense(num_decoder_tokens,activation=\"softmax\")\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([input_embedding_matrix])\n",
    "model.layers[2].trainable=False\n",
    "model.layers[3].set_weights([target_embedding_matrix])\n",
    "model.layers[3].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 18s 417ms/step - loss: 0.8012 - acc: 0.1467 - val_loss: 0.4068 - val_acc: 0.2144\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 16s 362ms/step - loss: 0.2908 - acc: 0.2441 - val_loss: 0.1653 - val_acc: 0.2448\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 16s 361ms/step - loss: 0.1307 - acc: 0.2743 - val_loss: 0.0829 - val_acc: 0.2515\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 16s 366ms/step - loss: 0.0750 - acc: 0.2796 - val_loss: 0.0615 - val_acc: 0.2546\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0563 - acc: 0.2807 - val_loss: 0.0504 - val_acc: 0.2563\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0496 - acc: 0.2817 - val_loss: 0.0465 - val_acc: 0.2561\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0470 - acc: 0.2802 - val_loss: 0.0491 - val_acc: 0.2753\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 16s 372ms/step - loss: 0.0457 - acc: 0.2818 - val_loss: 0.0451 - val_acc: 0.2555\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0451 - acc: 0.2820 - val_loss: 0.0431 - val_acc: 0.2552\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 16s 366ms/step - loss: 0.0445 - acc: 0.2816 - val_loss: 0.0435 - val_acc: 0.2553\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0442 - acc: 0.2818 - val_loss: 0.0431 - val_acc: 0.2565\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 16s 369ms/step - loss: 0.0442 - acc: 0.2829 - val_loss: 0.0443 - val_acc: 0.2569\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0439 - acc: 0.2830 - val_loss: 0.0468 - val_acc: 0.2752\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0438 - acc: 0.2829 - val_loss: 0.0429 - val_acc: 0.2574\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0449 - acc: 0.2816 - val_loss: 0.0429 - val_acc: 0.2552\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0437 - acc: 0.2825 - val_loss: 0.0431 - val_acc: 0.2546\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0426 - acc: 0.2829 - val_loss: 0.0396 - val_acc: 0.2571\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0412 - acc: 0.2832 - val_loss: 0.0397 - val_acc: 0.2575\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 16s 369ms/step - loss: 0.0388 - acc: 0.2833 - val_loss: 0.0407 - val_acc: 0.2769\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0407 - acc: 0.2827 - val_loss: 0.0372 - val_acc: 0.2591\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0359 - acc: 0.2850 - val_loss: 0.0357 - val_acc: 0.2583\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0324 - acc: 0.2857 - val_loss: 0.0324 - val_acc: 0.2603\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0291 - acc: 0.2880 - val_loss: 0.0305 - val_acc: 0.2607\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0262 - acc: 0.2897 - val_loss: 0.0282 - val_acc: 0.2636\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0225 - acc: 0.2912 - val_loss: 0.0300 - val_acc: 0.2831\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 16s 372ms/step - loss: 0.0205 - acc: 0.2903 - val_loss: 0.0224 - val_acc: 0.2650\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 16s 369ms/step - loss: 0.0182 - acc: 0.2928 - val_loss: 0.0216 - val_acc: 0.2642\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0163 - acc: 0.2920 - val_loss: 0.0196 - val_acc: 0.2643\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0145 - acc: 0.2922 - val_loss: 0.0194 - val_acc: 0.2652\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0124 - acc: 0.2944 - val_loss: 0.0169 - val_acc: 0.2674\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0119 - acc: 0.2952 - val_loss: 0.0220 - val_acc: 0.2850\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0099 - acc: 0.2943 - val_loss: 0.0197 - val_acc: 0.2668\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0097 - acc: 0.2961 - val_loss: 0.0149 - val_acc: 0.2660\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 16s 370ms/step - loss: 0.0081 - acc: 0.2956 - val_loss: 0.0136 - val_acc: 0.2668\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0078 - acc: 0.2951 - val_loss: 0.0118 - val_acc: 0.2676\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0078 - acc: 0.2957 - val_loss: 0.0159 - val_acc: 0.2666\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 16s 368ms/step - loss: 0.0065 - acc: 0.2962 - val_loss: 0.0122 - val_acc: 0.2882\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0100 - acc: 0.2956 - val_loss: 0.0168 - val_acc: 0.2672\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 16s 366ms/step - loss: 0.0098 - acc: 0.2937 - val_loss: 0.0115 - val_acc: 0.2673\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 16s 366ms/step - loss: 0.0061 - acc: 0.2967 - val_loss: 0.0105 - val_acc: 0.2673\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 16s 367ms/step - loss: 0.0045 - acc: 0.2974 - val_loss: 0.0111 - val_acc: 0.2680\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 16s 365ms/step - loss: 0.0047 - acc: 0.2963 - val_loss: 0.0094 - val_acc: 0.2696\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 16s 363ms/step - loss: 0.0041 - acc: 0.2972 - val_loss: 0.0084 - val_acc: 0.2890\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 16s 361ms/step - loss: 0.0086 - acc: 0.2949 - val_loss: 0.0172 - val_acc: 0.2669\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 16s 366ms/step - loss: 0.0085 - acc: 0.2954 - val_loss: 0.0087 - val_acc: 0.2689\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 16s 362ms/step - loss: 0.0044 - acc: 0.3003 - val_loss: 0.0108 - val_acc: 0.2681\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 16s 362ms/step - loss: 0.0039 - acc: 0.2969 - val_loss: 0.0076 - val_acc: 0.2688\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 16s 362ms/step - loss: 0.0031 - acc: 0.2976 - val_loss: 0.0063 - val_acc: 0.2704\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 16s 364ms/step - loss: 0.0070 - acc: 0.2960 - val_loss: 0.0104 - val_acc: 0.2890\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 16s 370ms/step - loss: 0.0045 - acc: 0.2966 - val_loss: 0.0081 - val_acc: 0.2700\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 16s 363ms/step - loss: 0.0030 - acc: 0.2975 - val_loss: 0.0087 - val_acc: 0.2689\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 16s 363ms/step - loss: 0.0031 - acc: 0.2960 - val_loss: 0.0059 - val_acc: 0.2693\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 16s 364ms/step - loss: 0.0025 - acc: 0.2976 - val_loss: 0.0062 - val_acc: 0.2695\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 18s 403ms/step - loss: 0.0029 - acc: 0.2976 - val_loss: 0.0073 - val_acc: 0.2702\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 19s 438ms/step - loss: 0.0025 - acc: 0.2972 - val_loss: 0.0083 - val_acc: 0.2898\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 17s 383ms/step - loss: 0.0020 - acc: 0.2975 - val_loss: 0.0053 - val_acc: 0.2706\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 16s 374ms/step - loss: 0.0015 - acc: 0.2988 - val_loss: 0.0052 - val_acc: 0.2699\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 16s 375ms/step - loss: 0.0016 - acc: 0.2988 - val_loss: 0.0058 - val_acc: 0.2696\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 16s 364ms/step - loss: 0.0017 - acc: 0.2985 - val_loss: 0.0057 - val_acc: 0.2699\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 16s 356ms/step - loss: 0.0014 - acc: 0.2975 - val_loss: 0.0058 - val_acc: 0.2712\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 16s 353ms/step - loss: 0.0018 - acc: 0.2980 - val_loss: 0.0062 - val_acc: 0.2908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "44/44 [==============================] - 19s 440ms/step - loss: 0.0015 - acc: 0.2984 - val_loss: 0.0047 - val_acc: 0.2711\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 22s 498ms/step - loss: 0.0020 - acc: 0.2981 - val_loss: 0.0054 - val_acc: 0.2699\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 16s 369ms/step - loss: 0.0013 - acc: 0.2969 - val_loss: 0.0049 - val_acc: 0.2696\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 16s 356ms/step - loss: 0.0015 - acc: 0.2971 - val_loss: 0.0063 - val_acc: 0.2698\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 16s 354ms/step - loss: 0.0021 - acc: 0.2973 - val_loss: 0.0055 - val_acc: 0.2709\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 16s 355ms/step - loss: 0.0012 - acc: 0.2971 - val_loss: 0.0049 - val_acc: 0.2909\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 16s 356ms/step - loss: 7.8868e-04 - acc: 0.2977 - val_loss: 0.0044 - val_acc: 0.2708\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 16s 358ms/step - loss: 0.0011 - acc: 0.2980 - val_loss: 0.0057 - val_acc: 0.2699\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 16s 358ms/step - loss: 7.1382e-04 - acc: 0.2982 - val_loss: 0.0064 - val_acc: 0.2694\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 16s 354ms/step - loss: 0.0015 - acc: 0.2963 - val_loss: 0.0064 - val_acc: 0.2698\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 16s 352ms/step - loss: 0.0012 - acc: 0.2984 - val_loss: 0.0070 - val_acc: 0.2708\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 0.0015 - acc: 0.2969 - val_loss: 0.0071 - val_acc: 0.2903\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 15s 352ms/step - loss: 0.0014 - acc: 0.2964 - val_loss: 0.0053 - val_acc: 0.2711\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 16s 356ms/step - loss: 6.9799e-04 - acc: 0.2981 - val_loss: 0.0052 - val_acc: 0.2700\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 17s 392ms/step - loss: 8.6232e-04 - acc: 0.2986 - val_loss: 0.0056 - val_acc: 0.2695\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 16s 354ms/step - loss: 7.7369e-04 - acc: 0.2970 - val_loss: 0.0070 - val_acc: 0.2698\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 7.0900e-04 - acc: 0.2987 - val_loss: 0.0061 - val_acc: 0.2711\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 15s 352ms/step - loss: 0.0011 - acc: 0.2976 - val_loss: 0.0052 - val_acc: 0.2909\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 15s 350ms/step - loss: 0.0011 - acc: 0.2972 - val_loss: 0.0057 - val_acc: 0.2709\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 6.5728e-04 - acc: 0.2979 - val_loss: 0.0050 - val_acc: 0.2699\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 16s 354ms/step - loss: 0.0012 - acc: 0.2978 - val_loss: 0.0055 - val_acc: 0.2695\n",
      "Epoch 83/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 8.5207e-04 - acc: 0.2983 - val_loss: 0.0154 - val_acc: 0.2687\n",
      "Epoch 84/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 0.0054 - acc: 0.2956 - val_loss: 0.0064 - val_acc: 0.2706\n",
      "Epoch 85/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 0.0016 - acc: 0.2979 - val_loss: 0.0064 - val_acc: 0.2905\n",
      "Epoch 86/100\n",
      "44/44 [==============================] - 15s 352ms/step - loss: 9.4508e-04 - acc: 0.2984 - val_loss: 0.0043 - val_acc: 0.2708\n",
      "Epoch 87/100\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 0.0011 - acc: 0.2974 - val_loss: 0.0040 - val_acc: 0.2701\n",
      "Epoch 88/100\n",
      "44/44 [==============================] - 17s 383ms/step - loss: 6.8808e-04 - acc: 0.2982 - val_loss: 0.0043 - val_acc: 0.2697\n",
      "Epoch 89/100\n",
      "44/44 [==============================] - 17s 389ms/step - loss: 0.0015 - acc: 0.2972 - val_loss: 0.0065 - val_acc: 0.2697\n",
      "Epoch 90/100\n",
      "44/44 [==============================] - 16s 369ms/step - loss: 0.0010 - acc: 0.2977 - val_loss: 0.0053 - val_acc: 0.2713\n",
      "Epoch 91/100\n",
      "44/44 [==============================] - 19s 424ms/step - loss: 6.6814e-04 - acc: 0.3013 - val_loss: 0.0055 - val_acc: 0.2909\n",
      "Epoch 92/100\n",
      "44/44 [==============================] - 16s 360ms/step - loss: 8.3301e-04 - acc: 0.2978 - val_loss: 0.0052 - val_acc: 0.2712\n",
      "Epoch 93/100\n",
      "44/44 [==============================] - 16s 360ms/step - loss: 0.0011 - acc: 0.2981 - val_loss: 0.0053 - val_acc: 0.2701\n",
      "Epoch 94/100\n",
      "44/44 [==============================] - 16s 358ms/step - loss: 6.0481e-04 - acc: 0.2978 - val_loss: 0.0047 - val_acc: 0.2698\n",
      "Epoch 95/100\n",
      "44/44 [==============================] - 16s 363ms/step - loss: 0.0015 - acc: 0.2974 - val_loss: 0.0053 - val_acc: 0.2699\n",
      "Epoch 96/100\n",
      "44/44 [==============================] - 17s 378ms/step - loss: 6.9019e-04 - acc: 0.2980 - val_loss: 0.0052 - val_acc: 0.2712\n",
      "Epoch 97/100\n",
      "44/44 [==============================] - 16s 364ms/step - loss: 6.2995e-04 - acc: 0.2966 - val_loss: 0.0065 - val_acc: 0.2908\n",
      "Epoch 98/100\n",
      "44/44 [==============================] - 16s 356ms/step - loss: 0.0012 - acc: 0.2980 - val_loss: 0.0049 - val_acc: 0.2710\n",
      "Epoch 99/100\n",
      "44/44 [==============================] - 16s 354ms/step - loss: 9.0837e-04 - acc: 0.2981 - val_loss: 0.0056 - val_acc: 0.2698\n",
      "Epoch 100/100\n",
      "44/44 [==============================] - 16s 360ms/step - loss: 0.0013 - acc: 0.2975 - val_loss: 0.0054 - val_acc: 0.2695\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "train_samples=len(X_train)\n",
    "val_samples=len(X_test)\n",
    "print(val_samples)\n",
    "epochs=100\n",
    "history=model.fit_generator(generate_batch(X_train,y_train,batch_size=batch_size),\n",
    "                   steps_per_epoch=train_samples//batch_size,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=generate_batch(X_test,y_test,batch_size=batch_size),\n",
    "                    validation_steps=val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference model to predict outputs\n",
    "#get the state given the input sequence \n",
    "encoder_model=Model(encoder_inputs,encoder_states)\n",
    "#decoder setup\n",
    "decoder_state_input_h=Input(shape=(embedding_size,))\n",
    "decoder_state_input_c=Input(shape=(embedding_size,))\n",
    "decoder_states_inputs=[decoder_state_input_h,decoder_state_input_c]\n",
    "decoder_emb2=decoder_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('lstmchat.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_word_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_word_index.items())\n",
    "def decode(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value=encoder_model.predict(input_seq)\n",
    "    # Generate empty target squence of length 1\n",
    "    target_seq=np.zeros((1,1))\n",
    "    #Give the first word of target sequence as start token\n",
    "    target_seq[0,0]=target_word_index[\"<SOS>\"]\n",
    "    \n",
    "    #sampling loop for a batch of sequences\n",
    "    stop=False\n",
    "    decoded_sent=\"\"\n",
    "    while not stop:\n",
    "        # we are predicting the final state vector of encoder i.e., {states_value}\n",
    "        # and initially target_seq will be start token <sos>\n",
    "        output_tokens,h,c=decoder_model.predict([target_seq]+states_value)\n",
    "        sampled_token_index=np.argmax(output_tokens[0,:,:])\n",
    "        #print(sampled_token_index)\n",
    "        sampled_word=reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sent+=\" \"+sampled_word\n",
    "        \n",
    "        # stop loop if <EOS> token is encountered or max length\n",
    "        if (sampled_word==\"<EOS>\" or len(decoded_sent)>1000):\n",
    "            stop=True\n",
    "        #update the target sequence to the previous output\n",
    "        target_seq=np.zeros((1,1))\n",
    "        target_seq[0,0]=sampled_token_index\n",
    "        states_value=[h,c]\n",
    "    return decoded_sent      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "print(input_seq.shape)\n",
    "decoded_sentence = decode(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Marathi Translation:', y_train[k:k+1].values[0][6:-5])\n",
    "print('Predicted Marathi Translation:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 41)\n",
      "Predicted Marathi Translation:  we do not have any physical store you can place your order here online url httpsmydreamstorein \n"
     ]
    }
   ],
   "source": [
    "inp=\"where is store\"\n",
    "input_seq=np.zeros((1,ileng),dtype=\"float32\")\n",
    "print(input_seq.shape)\n",
    "seq=[]\n",
    "for i,word in enumerate(inp.split()):\n",
    "    input_seq[0,i]=input_word_index[word]\n",
    "decoded_sentence = decode(input_seq)\n",
    "print('Predicted Marathi Translation:', decoded_sentence[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
